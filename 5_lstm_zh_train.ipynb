{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_text_to_id(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read().decode('utf8').replace(u'　', '').replace(u'\\n', '')\n",
    "        words = list(text)\n",
    "    vocab = ['<unk>'] + sorted(list(set(words)))\n",
    "    vocab = dict(zip(vocab, range(len(vocab))))\n",
    "    word_ids = [vocab[w] if w in vocab else 0 for w in words]\n",
    "    inv_vocab = np.array([x[1] for x in sorted(zip(vocab.values(), vocab.keys()))])\n",
    "    return word_ids, vocab, inv_vocab\n",
    "\n",
    "word_ids, vocab, inv_vocab = load_text_to_id('raw_novel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(word_ids, batch_size, n_steps):\n",
    "    word_ids = np.array(word_ids)\n",
    "    batch_count = len(word_ids) // batch_size\n",
    "    data = word_ids[:batch_count*batch_size].reshape([batch_size, batch_count])\n",
    "    for end in range(n_steps, batch_count, 1):\n",
    "        start = end - n_steps\n",
    "        x = data[:, start:end]\n",
    "        y = data[:, (start+1):(end+1)]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(scope_name, n_steps, dim_input, dim_hidden, batch_size, vocab_size, n_layer=1):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        input_data = tf.placeholder('int32', [batch_size, n_steps])\n",
    "        targets = tf.placeholder('int32', [batch_size, n_steps])\n",
    "        p_keep = tf.placeholder_with_default(tf.constant(1.0), [])\n",
    "        xavier = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        with tf.variable_scope(scope_name) as scope:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                try:\n",
    "                    embedding = tf.get_variable('embedding', [vocab_size, dim_input], initializer=xavier)\n",
    "                except ValueError:\n",
    "                    scope.reuse_variables()\n",
    "                    embedding = tf.get_variable('embedding', [vocab_size, dim_input], initializer=xavier)\n",
    "                inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, n_steps, inputs)]\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "                cell = tf.nn.rnn_cell.GRUCell(dim_hidden)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=p_keep)\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([cell] * n_layer)\n",
    "                initial_state = cell.zero_state(batch_size, 'float32')\n",
    "\n",
    "            outputs, state = tf.nn.rnn(cell, inputs, initial_state=initial_state)\n",
    "            output = tf.reshape(tf.concat(1, outputs), [-1, dim_hidden])\n",
    "            with tf.device('/gpu:0'):\n",
    "                Wy = tf.get_variable('Wy', [dim_hidden, vocab_size], initializer=xavier)\n",
    "                by = tf.get_variable('by', [vocab_size], initializer=xavier)\n",
    "                logits = tf.matmul(output, Wy) + by\n",
    "                probs = tf.nn.softmax(logits)\n",
    "                loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "                    [logits], [tf.reshape(targets, [-1])],\n",
    "                    [tf.ones([batch_size * n_steps], dtype='float32')], vocab_size)\n",
    "                cost = tf.reduce_sum(loss) / batch_size / n_steps\n",
    "                final_state = state\n",
    "                train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "    return {'train': train_op, 'final_state': final_state, 'cost': cost,\n",
    "            'logits': logits, 'input': input_data, 'target': targets,\n",
    "            'init_state': initial_state, 'cell': cell, 'p_keep': p_keep,\n",
    "            'embedding': embedding, 'probs': probs, 'graph': g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(n_steps, dim_input, dim_hidden, batch_size=128, n_layer=1, p_keep=1.0, final_train=False):\n",
    "    batch_size = 128\n",
    "    vocab_size = len(vocab)\n",
    "    if final_train:\n",
    "        scope_name = 'default'\n",
    "    else:\n",
    "        scope_name = 'rnn_{}_{}_{}_{}_{}'.format(n_layer, n_steps, dim_input, dim_hidden, batch_size)\n",
    "    model = get_model(scope_name, n_steps, dim_input, dim_hidden, batch_size, vocab_size, n_layer)\n",
    "    with model['graph'].as_default():\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            X = model['input']\n",
    "            Y = model['target']\n",
    "            dp = model['p_keep']\n",
    "            last_cost = 100.0\n",
    "            for epoch in range(100):\n",
    "                for x, y in batch(word_ids[:100000], batch_size, n_steps):\n",
    "                    sess.run(model['train'], feed_dict={X: x, Y: y, dp: p_keep})\n",
    "                cost = []\n",
    "                for x, y in batch(word_ids[100000:], batch_size, n_steps):\n",
    "                    cost.append(sess.run(model['cost'], feed_dict={X: x, Y: y, dp: 1.0}))\n",
    "                curr_cost = np.mean(cost)\n",
    "                # print epoch, curr_cost\n",
    "                if curr_cost > last_cost or abs(curr_cost - last_cost) < 0.01:\n",
    "                    break\n",
    "                last_cost = curr_cost\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            saver.save(sess, './lstm_zh.checkpoint')\n",
    "            return curr_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search (dim_input, dim_hidden, n_steps)\n",
    "\n",
    "1. number of dimensions of input\n",
    "2. number of hidden units\n",
    "3. number of RNN cells\n",
    "\n",
    "```python\n",
    "for dim_input in (5, 10, 15, 30, 50, 100):\n",
    "    for dim_hidden in (5, 10, 15, 30, 50, 100):\n",
    "        for n_steps in (20, 60, 100):\n",
    "            c = train(n_steps, dim_input, dim_hidden)\n",
    "            print 'dim_input={}, dim_hidden={}, n_steps={}, cost={}'.format(dim_input, dim_hidden, n_steps, c)\n",
    "```\n",
    "```\n",
    "dim_input=5, dim_hidden=5, n_steps=20, cost=5.04910373688\n",
    "dim_input=5, dim_hidden=5, n_steps=60, cost=5.00340223312\n",
    "dim_input=5, dim_hidden=5, n_steps=100, cost=4.9361448288\n",
    "dim_input=5, dim_hidden=10, n_steps=20, cost=4.67049598694\n",
    "dim_input=5, dim_hidden=10, n_steps=60, cost=4.60688018799\n",
    "dim_input=5, dim_hidden=10, n_steps=100, cost=4.68523263931\n",
    "dim_input=5, dim_hidden=15, n_steps=20, cost=4.48330354691\n",
    "dim_input=5, dim_hidden=15, n_steps=60, cost=4.536277771\n",
    "dim_input=5, dim_hidden=15, n_steps=100, cost=4.51421165466\n",
    "dim_input=5, dim_hidden=30, n_steps=20, cost=4.40719175339\n",
    "dim_input=5, dim_hidden=30, n_steps=60, cost=4.37579870224\n",
    "dim_input=5, dim_hidden=30, n_steps=100, cost=4.38251447678\n",
    "dim_input=5, dim_hidden=50, n_steps=20, cost=4.39374637604\n",
    "dim_input=5, dim_hidden=50, n_steps=60, cost=4.39796352386\n",
    "dim_input=5, dim_hidden=50, n_steps=100, cost=4.43117761612\n",
    "dim_input=5, dim_hidden=100, n_steps=20, cost=4.41315174103\n",
    "dim_input=5, dim_hidden=100, n_steps=60, cost=4.48978328705\n",
    "dim_input=5, dim_hidden=100, n_steps=100, cost=4.48827600479\n",
    "dim_input=10, dim_hidden=5, n_steps=20, cost=4.9605050087\n",
    "dim_input=10, dim_hidden=5, n_steps=60, cost=5.15566396713\n",
    "dim_input=10, dim_hidden=5, n_steps=100, cost=5.3531627655\n",
    "dim_input=10, dim_hidden=10, n_steps=20, cost=4.64183187485\n",
    "dim_input=10, dim_hidden=10, n_steps=60, cost=4.64913988113\n",
    "dim_input=10, dim_hidden=10, n_steps=100, cost=4.61222314835\n",
    "dim_input=10, dim_hidden=15, n_steps=20, cost=4.41376447678\n",
    "dim_input=10, dim_hidden=15, n_steps=60, cost=4.25506\n",
    "dim_input=10, dim_hidden=15, n_steps=100, cost=4.28174\n",
    "dim_input=10, dim_hidden=30, n_steps=20, cost=4.30813550949\n",
    "dim_input=10, dim_hidden=30, n_steps=60, cost=4.34905338287\n",
    "dim_input=10, dim_hidden=30, n_steps=100, cost=4.34371805191\n",
    "dim_input=10, dim_hidden=50, n_steps=20, cost=4.47641849518\n",
    "dim_input=10, dim_hidden=50, n_steps=60, cost=4.32932662964\n",
    "dim_input=10, dim_hidden=50, n_steps=100, cost=4.32593250275\n",
    "dim_input=10, dim_hidden=100, n_steps=20, cost=4.34707021713\n",
    "dim_input=10, dim_hidden=100, n_steps=60, cost=4.38002204895\n",
    "dim_input=10, dim_hidden=100, n_steps=100, cost=4.39789485931\n",
    "dim_input=15, dim_hidden=5, n_steps=20, cost=4.97037410736\n",
    "dim_input=15, dim_hidden=5, n_steps=60, cost=5.17431783676\n",
    "dim_input=15, dim_hidden=5, n_steps=100, cost=5.06800937653\n",
    "dim_input=15, dim_hidden=10, n_steps=20, cost=4.65017032623\n",
    "dim_input=15, dim_hidden=10, n_steps=60, cost=4.64081430435\n",
    "dim_input=15, dim_hidden=10, n_steps=100, cost=4.66324615479\n",
    "dim_input=15, dim_hidden=15, n_steps=20, cost=4.46678543091\n",
    "dim_input=15, dim_hidden=15, n_steps=60, cost=4.46504926682\n",
    "dim_input=15, dim_hidden=15, n_steps=100, cost=4.48022937775\n",
    "dim_input=15, dim_hidden=30, n_steps=20, cost=4.27884626389\n",
    "dim_input=15, dim_hidden=30, n_steps=60, cost=4.30791282654\n",
    "dim_input=15, dim_hidden=30, n_steps=100, cost=4.33599472046\n",
    "dim_input=15, dim_hidden=50, n_steps=20, cost=4.28652429581\n",
    "dim_input=15, dim_hidden=50, n_steps=60, cost=4.24734210968 <------- 1st best\n",
    "dim_input=15, dim_hidden=50, n_steps=100, cost=4.29611968994\n",
    "dim_input=15, dim_hidden=100, n_steps=20, cost=4.31619215012\n",
    "dim_input=15, dim_hidden=100, n_steps=60, cost=4.37390041351\n",
    "dim_input=15, dim_hidden=100, n_steps=100, cost=4.3488779068\n",
    "dim_input=30, dim_hidden=15, n_steps=20, cost=4.48592424393\n",
    "dim_input=30, dim_hidden=15, n_steps=60, cost=4.49895906448\n",
    "dim_input=30, dim_hidden=15, n_steps=100, cost=4.50793647766\n",
    "dim_input=30, dim_hidden=30, n_steps=20, cost=4.27560329437\n",
    "dim_input=30, dim_hidden=30, n_steps=60, cost=4.33786582947\n",
    "dim_input=30, dim_hidden=30, n_steps=100, cost=4.35293722153\n",
    "dim_input=30, dim_hidden=50, n_steps=20, cost=4.32201719284\n",
    "dim_input=30, dim_hidden=50, n_steps=60, cost=4.29839229584\n",
    "dim_input=30, dim_hidden=50, n_steps=100, cost=4.31720638275\n",
    "dim_input=30, dim_hidden=100, n_steps=20, cost=4.28649139404\n",
    "dim_input=30, dim_hidden=100, n_steps=60, cost=4.31956863403\n",
    "dim_input=30, dim_hidden=100, n_steps=100, cost=4.37884473801\n",
    "dim_input=50, dim_hidden=5, n_steps=20, cost=5.22038412094\n",
    "dim_input=50, dim_hidden=5, n_steps=60, cost=5.20120239258\n",
    "dim_input=50, dim_hidden=5, n_steps=100, cost=5.26643514633\n",
    "dim_input=50, dim_hidden=10, n_steps=20, cost=4.66747570038\n",
    "dim_input=50, dim_hidden=10, n_steps=60, cost=4.66478967667\n",
    "dim_input=50, dim_hidden=10, n_steps=100, cost=4.89870882034\n",
    "dim_input=50, dim_hidden=15, n_steps=20, cost=4.53129148483\n",
    "dim_input=50, dim_hidden=15, n_steps=60, cost=4.53673696518\n",
    "dim_input=50, dim_hidden=15, n_steps=100, cost=4.58493995667\n",
    "dim_input=50, dim_hidden=30, n_steps=20, cost=4.31741189957\n",
    "dim_input=50, dim_hidden=30, n_steps=60, cost=4.31924819946\n",
    "dim_input=50, dim_hidden=30, n_steps=100, cost=4.37507772446\n",
    "dim_input=50, dim_hidden=50, n_steps=20, cost=4.26547574997\n",
    "dim_input=50, dim_hidden=50, n_steps=60, cost=4.31258153915\n",
    "dim_input=50, dim_hidden=50, n_steps=100, cost=4.33610963821\n",
    "dim_input=50, dim_hidden=100, n_steps=20, cost=4.25190830231 <------- 2nd best\n",
    "dim_input=50, dim_hidden=100, n_steps=60, cost=4.30959272385\n",
    "dim_input=50, dim_hidden=100, n_steps=100, cost=4.45529270172\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search (n_layers, p_keep, n_batch)\n",
    "\n",
    "1. number of RNN layers\n",
    "2. probabilities of Dropout\n",
    "3. number of batches\n",
    "\n",
    "```python\n",
    "for n_layer in (1, 2, 3):\n",
    "    for p_keep in (0.5, 0.75, 1.0):\n",
    "        c = train(20, 50, 100, batch_size=64, n_layer=n_layer, p_keep=p_keep)\n",
    "        print '[20,50,100] n_layer={}, p_keep={}, cost={}'.format(n_layer, p_keep, c)\n",
    "        c = train(60, 15, 50, batch_size=128, n_layer=n_layer, p_keep=p_keep)\n",
    "        print '[60,15,50]  n_layer={}, p_keep={}, cost={}'.format(n_layer, p_keep, c)\n",
    "```\n",
    "```\n",
    "[20,50,100] n_layer=1, p_keep=0.5, cost=4.1572804451\n",
    "[60,15,50]  n_layer=1, p_keep=0.5, cost=4.17022037506\n",
    "[20,50,100] n_layer=1, p_keep=0.75, cost=4.12266349792 <-------- best\n",
    "[60,15,50]  n_layer=1, p_keep=0.75, cost=4.14526844025\n",
    "[20,50,100] n_layer=1, p_keep=1.0, cost=4.26864719391\n",
    "[60,15,50]  n_layer=1, p_keep=1.0, cost=4.24512195587\n",
    "[20,50,100] n_layer=2, p_keep=0.5, cost=5.92346096039\n",
    "[60,15,50]  n_layer=2, p_keep=0.5, cost=5.85321617126\n",
    "[20,50,100] n_layer=2, p_keep=0.75, cost=5.9269361496\n",
    "[60,15,50]  n_layer=2, p_keep=0.75, cost=5.8477640152\n",
    "[20,50,100] n_layer=2, p_keep=1.0, cost=4.47007656097\n",
    "[60,15,50]  n_layer=2, p_keep=1.0, cost=5.83855199814\n",
    "[20,50,100] n_layer=3, p_keep=0.5, cost=5.91980266571\n",
    "[60,15,50]  n_layer=3, p_keep=0.5, cost=5.85325241089\n",
    "[20,50,100] n_layer=3, p_keep=0.75, cost=5.90755319595\n",
    "[60,15,50]  n_layer=3, p_keep=0.75, cost=5.84755277634\n",
    "[20,50,100] n_layer=3, p_keep=1.0, cost=4.5511302948\n",
    "[60,15,50]  n_layer=3, p_keep=1.0, cost=5.83842325211\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final Training by Best Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1330943"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 當設 final_train=True 時，會將 variable scope 設為 'default' 以便後續使用\n",
    "train(20, 50, 100, 64, 1, 0.75, final_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
